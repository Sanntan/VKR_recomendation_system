# Тестирование ответов ассистента с использованием LLM

## Какой модуль тестируется и зачем
Пайплайн состоит из модуля `src/testing/llm_judge.py` и CLI-обёртки `scripts/run_llm_checks.py`. Он **не тестирует телеграм-бота или веб-парсер** напрямую. Вместо этого мы контролируем качество финального сообщения ассистента, которое попадает в описание PR или итоговое сообщение задачи в трекере. Цель — гарантировать, что ответ соответствует формату **Summary / Testing**, перечисляет сделанную работу и реально выполненные проверки. Так мы защищаемся от «пустых» отчётов ассистента, которые затем автоматически попадают в релизные заметки.

## Какие ответы проверяются
Мы оцениваем последнюю реплику ассистента. Пример — сообщение, которое бот отправляет в канал после обработки задачи:

```markdown
**Summary**
* Обновлён скрипт проверки

**Testing**
* ✅ `pytest tests/test_llm_judge.py`
```

Сообщение считается корректным, если соблюдены формальные правила и раздел Testing содержит реальные команды, подтверждающие валидацию. Важно понимать, что телеграм-бот и парсер в этот момент уже отработали: пайплайн проверяет **готовый текст**, а не исходный запрос пользователя.

## Формальные правила
Набор правил описан в `tests/validators/response_rules.yml` и делится на две группы: структурные требования и формат Testing. Таблица ниже помогает быстро понять, за что отвечает каждое правило:

| `rule_id` | Что проверяет | Зачем нужно |
| --- | --- | --- |
| `summary-header` | Есть ли строка `**Summary**` ровно в таком виде | Обозначает начало сводки, чтобы парсеры CI не путались |
| `summary-bullets` | Что сводка состоит из маркеров `* ` и не пуста | Заставляет ассистента перечислять изменения по пунктам |
| `testing-header` | Наличие строки `**Testing**` | Фиксирует отдельный раздел под проверки |
| `testing-emoji-bullets` | Каждый пункт Testing — маркер `* ` и статус-эмодзи (✅/⚠️/❌) | Даёт мгновенный визуальный статус каждого теста |
| `testing-command-format` | Команда в Testing оформлена в бэктиках | Позволяет копировать и повторно запускать команду без правок |

Статические проверки (`run_fallback_checks`) реализованы через регулярные выражения и применяются к любому тексту, поступающему на вход.

## Где и как используется ИИ
1. **Fallback-слой.** Если нарушено хотя бы одно правило из таблицы выше, проверка завершается с ошибкой без подключения LLM. Это видно в выводе `python scripts/run_llm_checks.py`: поле `Used LLM: no` означает, что сработал только статический уровень.
2. **LLM-слой.** Когда формальные проверки пройдены, класс `LLMJudge` строит промпт (`build_prompt`) из правил и ответа и передаёт его в клиент LLM. Теперь в проект встроена локальная альтернатива — `Ollama`. Помощник `create_ollama_client()` подключается к модели (по умолчанию `qwen2.5:0.5b`) через HTTP и заставляет LLM вынести смысловой вердикт: соответствует ли текст описанию правил, корректно ли перечислены проверки и есть ли конкретика в Summary.

> ⚠️ Если локальный сервер Ollama недоступен и `LLM_PROVIDER` явно не задан, пайплайн мягко откатывается к fallback-уровню. Но в CI мы требуем наличие LLM: задайте `LLM_PROVIDER=ollama` и убедитесь, что сервис запущен.

## Метрики валидации
- `passed` — итоговый флаг успешности проверки. Он равен `False`, если static-слой нашёл ошибки, либо переносится из ответа LLM.
- `failures` — нормализованный список нарушений. Каждый элемент содержит `rule_id` и `reason`, независимо от того, возникло нарушение в fallback или его нашла модель.
- `used_llm` — индикатор, который показывает, выполнялась ли LLM-верификация. Полезен в отчётах CI и при отладке.

Все поля собираются в `EvaluationResult.to_dict()` и могут сериализоваться в JSON для отчётов.

## Автоматические тесты
Юнит-тесты в `tests/test_llm_judge.py` целиком покрывают модуль `LLMJudge`:
- `test_build_prompt_includes_rules_and_response` убеждается, что промпт содержит и правила, и исходный ответ.
- `test_fallback_failure_prevents_llm_call` проверяет, что при структурной ошибке запрос к LLM не отправляется.
- `test_llm_judge_parses_successful_response` и `test_parse_llm_response_normalises_failures` валидируют обработку JSON-ответа модели.
- `test_run_fallback_checks_passes_for_valid_response` подтверждает, что эталонное сообщение проходит все проверки.

## Как запускать
```bash
python scripts/run_llm_checks.py --file tests/data/sample_llm_response.md
# или
pytest tests/test_llm_judge.py
```
Если нужно проверить реальный ответ ассистента, передайте текст через stdin или `--text`. Чтобы включить LLM-слой без ключей внешних провайдеров:

1. Установите [Ollama](https://ollama.com/), запустите `ollama serve` и загрузите модель: например, `ollama pull qwen2.5:0.5b`.
2. В окружении задайте `LLM_PROVIDER=ollama` (опционально `LLM_MODEL=qwen2.5:0.5b`).
3. Выполните `python scripts/run_llm_checks.py --text "..."` — скрипт автоматически создаст клиента через `create_ollama_client()`.

Если Ollama запущен на другом хосте, используйте `OLLAMA_BASE_URL`.

## Как расширять
- Добавляйте новые правила в `tests/validators/response_rules.yml` (поля `id`, `description`, `guidance`).
- Расширяйте CLI скрипт, если требуется автоматически подключать другие модели или API.
