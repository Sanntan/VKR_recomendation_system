# Тестирование ответов ассистента с использованием LLM

## Назначение пайплайна
Этот пайплайн автоматизирует проверку итоговых сообщений ассистента (например, описаний изменений в PR).
Мы проверяем, что ответ соответствует структуре, принятой в проекте:

- Заголовки **Summary** и **Testing** должны присутствовать и располагаться на отдельных строках.
- Списки в разделе Summary формируются через маркеры `* `.
- В разделе Testing каждая строка — это маркер `* `, статус-эмодзи (✅/⚠️/❌) и команда в бэктиках.

## Два уровня проверки
1. **Статические fallback-проверки** (`run_fallback_checks`).
   Используются регулярные выражения и простые правила, описанные в `tests/validators/response_rules.yml`.
   Эти проверки гарантируют структуру ответа и экономят запрос к языковой модели.
2. **LLM-проверка** (`LLMJudge`).
   Если статические проверки прошли, формируется промпт (функция `build_prompt`) с перечислением правил и исходным ответом.
   Далее вызывается LLM-клиент, который возвращает JSON следующего вида:
   ```json
   {
     "passed": true | false,
     "failures": [
       {"rule_id": "summary-bullets", "reason": "Нет маркированного списка"}
     ]
   }
   ```
   Ответ преобразуется в `EvaluationResult`, где дополнительно сохраняется исходный ответ модели и признак `used_llm`.

## Метрики валидации и верификации
- `passed` — итоговый флаг успешности. Он вычисляется либо на этапе fallback (если есть ошибки, возвращаем `False`), либо переносится из ответа LLM.
- `failures` — список нарушений. Для статических проверок мы формируем его сами, для LLM — нормализуем через `parse_llm_response`, чтобы каждое нарушение содержало `rule_id` и `reason`.
- `used_llm` — индикатор, что к проверке привлекалась LLM. Он позволяет отличать быстрые статические проверки от полноценных LLM-верификаций.

Эти поля сериализуются методом `EvaluationResult.to_dict()` и могут использоваться в внешних отчётах или CI.

## Как используется ИИ
- Статические проверки проверяют только синтаксис.
- LLM оценивает смысловые аспекты: соответствие описанию правил, корректность содержания Testing, наличие конкретики.
  Мы явно передаём правила в промпте, поэтому модель работает как «судья», а не как генератор нового текста.

## Автоматические тесты
Модульные тесты в `tests/test_llm_judge.py` проверяют:
- Формирование промпта (`test_build_prompt_includes_rules_and_response`).
- Поведение fallback-проверок и отсутствие лишних запросов к LLM (`test_fallback_failure_prevents_llm_call`).
- Парсинг успешного ответа модели и нормализацию списка ошибок (`test_llm_judge_parses_successful_response`, `test_parse_llm_response_normalises_failures`).
- Успешное прохождение валидного примера (`test_run_fallback_checks_passes_for_valid_response`).

## Как запускать
```bash
python scripts/run_llm_checks.py --file tests/data/sample_llm_response.md
# или
pytest tests/test_llm_judge.py
```
Если нужно проверить реальный ответ ассистента, передайте текст через stdin или `--text`.

## Доработки
- Добавлено описание пайплайна на русском языке (этот файл).
- При необходимости можно расширять `tests/validators/response_rules.yml`, добавляя новые правила с полями `id`, `description`, `guidance`.
