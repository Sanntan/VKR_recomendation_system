# Тестирование ответов ассистента с использованием LLM

## Какой модуль тестируется и зачем
Пайплайн состоит из модуля `src/testing/llm_judge.py` и CLI-обёртки `scripts/run_llm_checks.py`. Он проверяет финальные ответы ассистента, которые попадают в описание PR или итоговое сообщение задачи. Цель — гарантировать, что сообщение соответствует принятому в команде формату **Summary / Testing** и действительно перечисляет выполненную работу и прогнанные проверки.

## Какие ответы проверяются
Мы оцениваем строку с финальным сообщением ассистента, например:

```markdown
**Summary**
* Обновлён скрипт проверки

**Testing**
* ✅ `pytest tests/test_llm_judge.py`
```

Сообщение считается корректным, если соблюдены формальные правила и Testing содержит реальные команды, подтверждающие валидацию.

## Формальные правила
Набор правил описан в `tests/validators/response_rules.yml` и делится на две группы: структурные требования и формат Testing. Таблица ниже помогает быстро понять, за что отвечает каждое правило:

| `rule_id` | Что проверяет | Зачем нужно |
| --- | --- | --- |
| `summary-header` | Есть ли строка `**Summary**` ровно в таком виде | Обозначает начало сводки, чтобы парсеры CI не путались |
| `summary-bullets` | Что сводка состоит из маркеров `* ` и не пуста | Заставляет ассистента перечислять изменения по пунктам |
| `testing-header` | Наличие строки `**Testing**` | Фиксирует отдельный раздел под проверки |
| `testing-emoji-bullets` | Каждый пункт Testing — маркер `* ` и статус-эмодзи (✅/⚠️/❌) | Даёт мгновенный визуальный статус каждого теста |
| `testing-command-format` | Команда в Testing оформлена в бэктиках | Позволяет копировать и повторно запускать команду без правок |

Статические проверки (`run_fallback_checks`) реализованы через регулярные выражения и применяются к любому тексту, поступающему на вход.

## Где и как используется ИИ
1. **Fallback-слой.** Если нарушено хотя бы одно правило из таблицы выше, проверка завершается с ошибкой без подключения LLM. Это видно в выводе `python scripts/run_llm_checks.py`: поле `Used LLM: no` означает, что сработал только статический уровень.
2. **LLM-слой.** Когда формальные проверки пройдены, класс `LLMJudge` строит промпт (`build_prompt`) из правил и ответа и передаёт его в клиент LLM. На проекте для смысловой верификации предусмотрена модель `gpt-4o-mini` (её имя передаётся через окружение `LLM_MODEL`), потому что она хорошо справляется с проверкой фактов и формата Testing. К клиенту обращаются функции `judge()` / `_call_client()`. Возвращаемый JSON содержит `passed` и `failures`, что позволяет понять, соблюдены ли смысловые требования: соответствие описанию правил, корректность Testing и наличие конкретики в Summary.

> ⚠️ Если ключ API или модель не заданы, LLM не вызывается — пайплайн работает только на fallback-проверках. Это полезно для локальной разработки без доступа к внешним сервисам.

## Метрики валидации
- `passed` — итоговый флаг успешности проверки. Он равен `False`, если static-слой нашёл ошибки, либо переносится из ответа LLM.
- `failures` — нормализованный список нарушений. Каждый элемент содержит `rule_id` и `reason`, независимо от того, возникло нарушение в fallback или его нашла модель.
- `used_llm` — индикатор, который показывает, выполнялась ли LLM-верификация. Полезен в отчётах CI и при отладке.

Все поля собираются в `EvaluationResult.to_dict()` и могут сериализоваться в JSON для отчётов.

## Автоматические тесты
Юнит-тесты в `tests/test_llm_judge.py` целиком покрывают модуль `LLMJudge`:
- `test_build_prompt_includes_rules_and_response` убеждается, что промпт содержит и правила, и исходный ответ.
- `test_fallback_failure_prevents_llm_call` проверяет, что при структурной ошибке запрос к LLM не отправляется.
- `test_llm_judge_parses_successful_response` и `test_parse_llm_response_normalises_failures` валидируют обработку JSON-ответа модели.
- `test_run_fallback_checks_passes_for_valid_response` подтверждает, что эталонное сообщение проходит все проверки.

## Как запускать
```bash
python scripts/run_llm_checks.py --file tests/data/sample_llm_response.md
# или
pytest tests/test_llm_judge.py
```
Если нужно проверить реальный ответ ассистента, передайте текст через stdin или `--text`. Для подключения LLM задайте переменные окружения `OPENAI_API_KEY` и `LLM_MODEL=gpt-4o-mini`, а затем создайте клиента и передайте его в `LLMJudge`, например:

```python
import os

from openai import OpenAI
from src.testing.llm_judge import LLMJudge

client = OpenAI()
judge = LLMJudge(client=lambda prompt: client.chat.completions.create(
    model=os.environ.get("LLM_MODEL", "gpt-4o-mini"),
    messages=[{"role": "user", "content": prompt}],
).choices[0].message.content)
```

## Как расширять
- Добавляйте новые правила в `tests/validators/response_rules.yml` (поля `id`, `description`, `guidance`).
- Расширяйте CLI скрипт, если требуется автоматически подключать другие модели или API.
