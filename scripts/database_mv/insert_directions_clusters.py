from pathlib import Path
import pandas as pd
import numpy as np
from sqlalchemy.orm import Session
from sqlalchemy import select, delete
from sentence_transformers import SentenceTransformer
import hdbscan

from src.core.database.connection import engine
from src.core.database.models import Directions, Clusters
from scripts.database_mv.preprocess_excel import preprocess_excel


# === –ü—É—Ç–∏ ===
BASE_DIR = Path(__file__).resolve().parent
RESULTS_DIR = BASE_DIR / "results"
FILTERED_FILE = RESULTS_DIR / "filtered_data.xlsx"

# === –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ===
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
EMBED_DIM = model.get_sentence_embedding_dimension()  # –æ–±—ã—á–Ω–æ 384

def clusterize_directions(directions: list[str]):
    """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
    print(f"üß† –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(directions)} –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π...")
    embeddings = model.encode(directions, normalize_embeddings=True, show_progress_bar=True)

    print("üîç –ó–∞–ø—É—Å–∫ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, metric="euclidean")
    labels = clusterer.fit_predict(embeddings)

    # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (-1)
    unique_labels = [lbl for lbl in set(labels) if lbl != -1]
    next_cluster_id = max(unique_labels, default=-1) + 1

    final_labels = []
    for lbl in labels:
        if lbl == -1:
            final_labels.append(next_cluster_id)
            next_cluster_id += 1
        else:
            final_labels.append(lbl)

    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(set(final_labels))} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–≤–∫–ª—é—á–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ)")

    df_clusters = pd.DataFrame({
        "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": directions,
        "–ö–ª–∞—Å—Ç–µ—Ä": final_labels,
        "–¢–∏–ø": ["–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π" if l >= len(unique_labels) else "–æ—Å–Ω–æ–≤–Ω–æ–π" for l in final_labels]
    })

    return df_clusters, embeddings, final_labels


def insert_clusters_and_directions(df_clusters: pd.DataFrame, embeddings: np.ndarray, final_labels: list[int]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É"""
    with Session(engine) as db:
        # === –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π ===
        db.execute(delete(Directions))
        db.execute(delete(Clusters))
        db.commit()

        # === –°–æ–∑–¥–∞—ë–º –∫–ª–∞—Å—Ç–µ—Ä—ã ===
        cluster_ids = {}
        for cluster_label in sorted(set(final_labels)):
            indices = [i for i, lbl in enumerate(final_labels) if lbl == cluster_label]
            cluster_vectors = embeddings[indices]
            centroid = cluster_vectors.mean(axis=0)
            # –ø–∞–¥–¥–∏–Ω–≥ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if len(centroid) < EMBED_DIM:
                centroid = np.pad(centroid, (0, EMBED_DIM - len(centroid)))
            title = f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_label + 1}"
            cluster = Clusters(title=title, centroid=centroid.tolist())
            db.add(cluster)
            db.commit()
            db.refresh(cluster)
            cluster_ids[cluster_label] = cluster.id

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_ids)}")

        # === –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è ===
        for idx, row in df_clusters.iterrows():
            cluster_id = cluster_ids.get(row["–ö–ª–∞—Å—Ç–µ—Ä"])
            db.add(Directions(title=row["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"], cluster_id=cluster_id))
        db.commit()

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(df_clusters)}")


def main():
    print("üîÑ –®–∞–≥ 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ Excel...")
    preprocess_excel()

    if not FILTERED_FILE.exists():
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª {FILTERED_FILE}")

    df = pd.read_excel(FILTERED_FILE)
    if "–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å" not in df.columns:
        raise ValueError("‚ùå –í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü '–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å'")

    directions = (
        df["–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å"]
        .dropna()
        .astype(str)
        .str.strip()
        .loc[lambda s: s != ""]
        .drop_duplicates()
        .tolist()
    )

    print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(directions)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π")

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ–≤—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è ===
    with Session(engine) as db:
        existing_titles = {d.title.lower() for d in db.scalars(select(Directions.title)).all()}
        new_dirs = [d for d in directions if d.lower() not in existing_titles]

    if not existing_titles:
        print("üÜï –ë–î –ø—É—Å—Ç–∞. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–≤—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
    elif new_dirs:
        print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(new_dirs)} –Ω–æ–≤—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–ª–∞—Å—Ç–µ—Ä—ã...")
    else:
        print("‚úÖ –ù–æ–≤—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –Ω–µ—Ç. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return

    # === –ó–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –≤—Å—Ç–∞–≤–∫–∏ ===
    df_clusters, embeddings, final_labels = clusterize_directions(directions)
    insert_clusters_and_directions(df_clusters, embeddings, final_labels)


if __name__ == "__main__":
    main()
